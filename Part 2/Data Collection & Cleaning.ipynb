{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before we can commence imports, we need to ensure we have the following modules downloaded:\n",
    "!pip install fake-useragent   #allows a fake user agent to be specified for each call to Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import html  \n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Product Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I intend to perform natural language processing to establish a model to predict the star rating an Amazon customer will leave based on the review they leave a product. \n",
    "\n",
    "The star rating here is unique to each review, and is the score as defined by the customer. This is not the overall star rating that a product has on Amazon. The overall rating is defined by Amazon: \n",
    "\n",
    "\"Amazon calculates a product’s star ratings using a machine learned model instead of a raw data average. The machine learned model takes into account factors including: the age of a review, helpfulness votes by customers and whether the reviews are from verified purchases.\"\n",
    "\n",
    "As an extension, I intend to establish a model to predict whether another Amazon customer would deem a review 'helpful' based on its content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Information Do I Need To Scrape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will need to do two rounds of scraping, Primarily to get all of the product links, and then once I have the products, I want to scrape all of their customer reviews.\n",
    "\n",
    "I will create one script to run on pages that look like the below image, where I will take:\n",
    "1. ASIN - The unique product identifier for Amazon Products\n",
    "2. Product Name\n",
    "3. Price\n",
    "4. Number of Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Product List](images/product_list.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I have Run this scrape on a number of different product types, I will then for each product scrape the product reviews as left by customers.\n",
    "\n",
    "The features that I will take from the reviews are:\n",
    "\n",
    "1. Star rating\n",
    "2. Review title\n",
    "3. Author\n",
    "4. Date review was left\n",
    "5. Whether purchase was verified or not\n",
    "6. Review\n",
    "7. Helpful vote\n",
    "\n",
    "Additional Features\n",
    "\n",
    "8. Whether the review has any images\n",
    "9. Whether the review has any videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Review Text](images/review_text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Scrapes\n",
    "\n",
    "Here I will define function to take all the products from a product grid, and save the features to a dictionary. The function will take all the products on a page, and then navigate to the next page and take thos products and continue in this way until there are no further pages to visit for this scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_products(page, prod_dict):\n",
    "    \n",
    "    \n",
    "    #XPATHS\n",
    "    #xpath for amazon product list item\n",
    "    xpath_product = \"//li[starts-with(@id, 'result')]\"\n",
    "    #xpath for next product arrow link\n",
    "    xpath_next_page = \"//a[@id='pagnNextLink']/@href\"\n",
    "\n",
    "    #set up the request\n",
    "    headers = {'User-Agent': ua.random}\n",
    "    r = requests.get(page, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        print('status error')\n",
    "\n",
    "    #get test response from request\n",
    "    products_page = r.text\n",
    "    \n",
    "    #parse the page\n",
    "    parser = html.fromstring(products_page)\n",
    "    \n",
    "    \n",
    "    # get the individual products  \n",
    "    products = parser.xpath(xpath_product)\n",
    "    \n",
    "    \n",
    "    #iterate through products and get their info\n",
    "    for product in products:\n",
    "        \n",
    "        # take the amazon asin unique identifier\n",
    "        try:\n",
    "            prod_dict['asins'].append(product.xpath(\"@data-asin\")[0])\n",
    "        except:\n",
    "            prod_dict['asins'].append(\"none\")\n",
    "            \n",
    "            \n",
    "        # take the product title    \n",
    "        try:\n",
    "            prod_dict['titles'].append(\n",
    "                product.xpath(\"div/div[3 or 4]/div[1]/a/h2/text()\")[0])\n",
    "        except:\n",
    "            prod_dict['titles'].append(\"none\")\n",
    "            \n",
    "            \n",
    "        #take the number of reviews (this can be in 2 places based on product detail)\n",
    "        try:\n",
    "            # if 1st found element has 5 or more characters it is a 'more products' link, so take 2nd element\n",
    "            if len(\n",
    "                    product.xpath(\n",
    "                        \"div/div[6 or 7]/a[starts-with(@class, 'a-size-small')]/text()\"\n",
    "                    )[0]) > 5:\n",
    "                prod_dict['review_counts'].append(\n",
    "                    int(\n",
    "                        product.xpath(\n",
    "                            \"div/div[6 or 7]/a[starts-with(@class, 'a-size-small')]/text()\"\n",
    "                        )[1]))\n",
    "            else:\n",
    "                prod_dict['review_counts'].append(\n",
    "                    int(\n",
    "                        product.xpath(\n",
    "                            \"div/div[6 or 7]/a[starts-with(@class, 'a-size-small')]/text()\"\n",
    "                        )[0]))\n",
    "        except:\n",
    "            prod_dict['review_counts'].append(0)\n",
    "            \n",
    "            \n",
    "        # take the price of the product (can appear in 1 of 2 places)    \n",
    "        try:\n",
    "            if len(product.xpath(\"div/div[5 or 7]/div[1]/a/span/text()\")[\n",
    "                    0]) == 0:\n",
    "                prod_dict['prices'].append(\n",
    "                    (\n",
    "                        product.xpath(\"div/div[5 or 7]/div[1]/a/span/text()\")[\n",
    "                            1].replace(\"£\", \"\").replace(\",\", \"\")))\n",
    "            else:\n",
    "                prod_dict['prices'].append(\n",
    "                    (\n",
    "                        product.xpath(\"div/div[5 or 7]/div[1]/a/span/text()\")[\n",
    "                            0].replace(\"£\", \"\").replace(\",\", \"\")))\n",
    "        except:\n",
    "            prod_dict['prices'].append(np.nan)\n",
    "            \n",
    "            \n",
    "    # get the next page from the arrow link\n",
    "    next_page = parser.xpath(xpath_next_page)\n",
    "    \n",
    "    #if no more pages, return the dictionary\n",
    "    if len(next_page) == 0:\n",
    "        return prod_dict\n",
    "    \n",
    "    #otherwise go to next page and repeat \n",
    "    else:\n",
    "        page = \"https://www.amazon.co.uk\" + next_page[0]\n",
    "        return get_products(page, prod_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function can then be called to start running on any page that includes the products in a grid as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thr function needs a dictionary to store the features\n",
    "prod_dict = {'asins':[],\n",
    "            'titles':[],\n",
    "            'review_counts':[],\n",
    "            'prices':[]}\n",
    "\n",
    "# This page will return all products in the beers,wines and spirits category\n",
    "page = \"https://www.amazon.co.uk/beer-wine-spirits/b/ref=nav_shopall_wine_spirits?ie=UTF8&node=358583031\"\n",
    "\n",
    "#The function will return a dictionary called bws\n",
    "bws = get_products(page,prod_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results can then be loaded into a dataframe for inspection\n",
    "df = pd.DataFrame(bws,columns=bws.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the product information to a .csv file\n",
    "df.to_csv('amazon_bws.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Product Searched\n",
    "\n",
    "I have taken 4 different groups of products to look at the reviews for:\n",
    "\n",
    "1. Digital SLR cameras\n",
    "2. Mens sport shoes\n",
    "3. Beers/wines/spirits\n",
    "4. Playmobil\n",
    "\n",
    "I have chosen playmobil here as a product type for which the review most likely isnt being written by the end user and I expect that there may be some interesting results from this.\n",
    "\n",
    "I am expecting the nlp models to establish different feature weights for each category, but would like to look at the features that are shared between categories. I expect that features shared across categories may give an indication of service as opposed to product. This could be a useful indicator for Amazon Customer service to monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD RESULTS FROM PRODUCT SCRAPES:\n",
    "\n",
    "df_slr = pd.read_csv('amazon_slr.csv')\n",
    "df_bws = pd.read_csv('amazon_bws.csv')\n",
    "df_mens_shoes = pd.read_csv('amazon_mens_shoes.csv')\n",
    "df_playmobil = pd.read_csv('amazon_playmobil.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Playmobil as an example\n",
    "df_playmobil = pd.read_csv('amazon_playmobil.csv')\n",
    "df_playmobil.drop(\"Unnamed: 0\",inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the number of playmobil products returned\n",
    "len(df_playmobil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asins</th>\n",
       "      <th>titles</th>\n",
       "      <th>review_counts</th>\n",
       "      <th>prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00VGQKBGK</td>\n",
       "      <td>PLAYMOBIL Take Along Pet Store Playset</td>\n",
       "      <td>2</td>\n",
       "      <td>24.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B01608M23I</td>\n",
       "      <td>Playmobil 6888 Summer Fun Camp Site with LED Fire</td>\n",
       "      <td>26</td>\n",
       "      <td>14.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00IF1VVFO</td>\n",
       "      <td>Playmobil 5568 City Life Preschool Children's ...</td>\n",
       "      <td>162</td>\n",
       "      <td>17.60 - 73.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B01LTHZP16</td>\n",
       "      <td>Playmobil 6921 City Action Police Helicopter w...</td>\n",
       "      <td>51</td>\n",
       "      <td>18.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00VLUZ31O</td>\n",
       "      <td>Playmobil 6657 City Life Furnished Children's ...</td>\n",
       "      <td>91</td>\n",
       "      <td>59.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        asins                                             titles  \\\n",
       "0  B00VGQKBGK             PLAYMOBIL Take Along Pet Store Playset   \n",
       "1  B01608M23I  Playmobil 6888 Summer Fun Camp Site with LED Fire   \n",
       "2  B00IF1VVFO  Playmobil 5568 City Life Preschool Children's ...   \n",
       "3  B01LTHZP16  Playmobil 6921 City Action Police Helicopter w...   \n",
       "4  B00VLUZ31O  Playmobil 6657 City Life Furnished Children's ...   \n",
       "\n",
       "   review_counts         prices  \n",
       "0              2          24.99  \n",
       "1             26          14.99  \n",
       "2            162  17.60 - 73.84  \n",
       "3             51          18.49  \n",
       "4             91          59.99  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of product listing\n",
    "df_playmobil.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I know from the first scrape how many reviews a product has, and that each product review page has 10 reviews, I can utilise the power of parallel requests to scrape reveiews in parallel rather than in a sequential manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules for multi threading\n",
    "import multiprocessing as mp\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for picking out the salient details from a review block\n",
    "\n",
    "\n",
    "def get_asin(review):\n",
    "    xpath_asin = \".//a[@data-hook='review-title']/@href\"\n",
    "    return review.xpath(xpath_asin)[0][-10:]\n",
    "\n",
    "def get_review_id(review):\n",
    "    return review.xpath(\"@id\")[0]\n",
    "\n",
    "\n",
    "def get_stars(review):\n",
    "    xpath_stars = \".//i[@data-hook='review-star-rating']//text()\"\n",
    "    return review.xpath(xpath_stars)[0][0]\n",
    "\n",
    "\n",
    "def get_title(review):\n",
    "    xpath_title = \".//a[@data-hook='review-title']//text()\"\n",
    "    return review.xpath(xpath_title)[0]\n",
    "\n",
    "\n",
    "def get_comment(review):\n",
    "    xpath_comment = \".//span[@data-hook='review-body']//text()\"\n",
    "    if review.xpath(xpath_comment) != []:\n",
    "        return review.xpath(xpath_comment)[0]\n",
    "    else: \n",
    "        return \"QQQQQQQQQ\" \n",
    "\n",
    "\n",
    "def get_author(review):\n",
    "    xpath_author = \".//a[@data-hook='review-author']/@href\"\n",
    "    if review.xpath(xpath_author) != [] and len(review.xpath(xpath_author)[0]) > 26:\n",
    "        return review.xpath(xpath_author)[0][26:]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_date(review):\n",
    "    xpath_date = \".//span[@data-hook='review-date']//text()\"\n",
    "    return review.xpath(xpath_date)[0][3:]\n",
    "\n",
    "\n",
    "def get_verified(review):\n",
    "    xpath_verified = \".//span[@data-hook='avp-badge']//text()\"\n",
    "    if review.xpath(xpath_verified) != []:\n",
    "        return review.xpath(xpath_verified)[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_helpful_count(review):\n",
    "    xpath_helpful = \".//span[@data-hook='helpful-vote-statement']//text()\"\n",
    "    if review.xpath(xpath_helpful) != []:\n",
    "        score = review.xpath(xpath_helpful)[0].split()[0]\n",
    "        if score == \"One\":\n",
    "            return 1\n",
    "        else:\n",
    "            return score\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_image_count(review):\n",
    "    xpath_image = \".//img[@data-hook='review-image-tile']\"\n",
    "    if review.xpath(xpath_image) != []:\n",
    "        return len(review.xpath(xpath_image))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_author_status(review):\n",
    "    xpath_status = \".//span[@data-hook='review-author']/following-sibling::span[@class='a-size-mini a-color-link c7yBadgeAUI c7yTopDownDashedStrike c7y-badge-text a-text-bold']/text()\"\n",
    "    if review.xpath(xpath_status) != []:\n",
    "        return review.xpath(xpath_status)[0]\n",
    "    else:\n",
    "        return \"none\"\n",
    "    \n",
    "def get_video_block(review):\n",
    "    xpath_video = \"div/div/span/div[starts-with(@id,'video-block')]\"\n",
    "    if review.xpath(xpath_video) != []:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will define the function to go through each review on a review page, and extract the features to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_2(page):\n",
    "    \n",
    "    \n",
    "    review_dict = {\n",
    "    'asin': [],\n",
    "    'page': [],\n",
    "    'stars' : [],\n",
    "    'author': [],\n",
    "    'date': [],\n",
    "    'title':[],\n",
    "    'comment': [],\n",
    "    'verified': [],\n",
    "    'helpful': [],\n",
    "    'pics': [],\n",
    "    'video': [],\n",
    "    'comment_id': [],\n",
    "    'author_status':[]\n",
    "    }\n",
    "    \n",
    "    #set up the request\n",
    "    headers = {'User-Agent': ua.safari}\n",
    "    r = requests.get(page, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        print('status error',r.status_code,page)\n",
    "\n",
    "    #get test response from request\n",
    "    reviews_page = r.text\n",
    "\n",
    "    #parse the page\n",
    "    parser = html.fromstring(reviews_page)\n",
    "\n",
    "    # get the individual products\n",
    "    xpath_review = \"//div[@data-hook='review']\"\n",
    "    reviews = parser.xpath(xpath_review)\n",
    "\n",
    "    for review in reviews:\n",
    "        #add returned values to the list within the dictionary\n",
    "        review_dict['asin'].append(get_asin(review))\n",
    "        review_dict['page'].append(page)\n",
    "        review_dict['stars'].append(get_stars(review))\n",
    "        review_dict['title'].append(get_title(review))\n",
    "        review_dict['comment'].append(get_comment(review))\n",
    "        review_dict['author'].append(get_author(review))\n",
    "        review_dict['date'].append(get_date(review))\n",
    "        review_dict['comment_id'].append(get_review_id(review))\n",
    "        review_dict['verified'].append(get_verified(review))\n",
    "        review_dict['helpful'].append(get_helpful_count(review))\n",
    "        review_dict['author_status'].append(get_author_status(review))\n",
    "        review_dict['pics'].append(get_image_count(review))\n",
    "        review_dict['video'].append(get_video_block(review))\n",
    "    \n",
    "    print(review_dict)\n",
    "#     return_dict[page]=review_dict\n",
    "    return review_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the multi threading, I will loop through my products dataframe, and for each product, generate a request for the appropriate number of review page scrapes based on the 10 reviews per page fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def async_get(urls):\n",
    "    ls_=[]\n",
    "    pool = ThreadPool(18)\n",
    "    results = pool.map_async(get_reviews_2, urls)\n",
    "    results.wait()\n",
    "    ls_.append(results.get())\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return ls_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Reviews\n",
    "\n",
    "For Each of the product dataframes, I will run the above process, and then process the resulting dictionary of dictionaries into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Run of the above process\n",
    "start = time.time()\n",
    "\n",
    "playmobil_reviews = async_get(urls,master_dict)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the dictionary of dictionaries into a dataframe\n",
    "review_dict = {\n",
    "    'asin': [],\n",
    "    'page': [],\n",
    "    'stars' : [],\n",
    "    'author': [],\n",
    "    'date': [],\n",
    "    'title':[],\n",
    "    'comment': [],\n",
    "    'verified': [],\n",
    "    'helpful': [],\n",
    "    'pics': [],\n",
    "    'video': [],\n",
    "    'comment_id': [],\n",
    "    'author_status':[]\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(columns = review_dict.keys())\n",
    "\n",
    "for i in range(len(t__[0])):\n",
    "    df = df.append(pd.DataFrame(playmobil_reviews[0][i],columns=review_dict.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading some sample reviews\n",
    "example_reviews = pd.read_csv('example_reviews.csv')\n",
    "example_reviews.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>page</th>\n",
       "      <th>stars</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>verified</th>\n",
       "      <th>helpful</th>\n",
       "      <th>pics</th>\n",
       "      <th>video</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B01608M23I</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>AHYW7TVACEMYQS2NQ2D3CISJQLYQ</td>\n",
       "      <td>31 October 2017</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Happy with product he loves play mobile.</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RTTUM0QU7HZWQ</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B01608M23I</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>AGYJQ2G2AUWNUJRB7SVFRV3PSJ6Q</td>\n",
       "      <td>17 January 2018</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>excellent</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>R3FNSJ45BRF0X8</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B01608M23I</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>AFA7TYS6KM6QLO5USYDOIDKUAMTA</td>\n",
       "      <td>16 March 2018</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Grand daughter loves it</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RE2CU061L9FBT</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B01608M23I</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>AHKXO7XI2FSQYJEUID7H5TRMY4ZQ</td>\n",
       "      <td>27 March 2018</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Godson loves it</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>R1G8XD7M3NPF9M</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B01608M23I</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>AHA2E7FB7Y6QQQPZNM62R4CNACSA</td>\n",
       "      <td>21 March 2018</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>R2YW9S0RZDA23M</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  page  stars                        author             date  \\\n",
       "0  B01608M23I     3      5  AHYW7TVACEMYQS2NQ2D3CISJQLYQ  31 October 2017   \n",
       "1  B01608M23I     3      5  AGYJQ2G2AUWNUJRB7SVFRV3PSJ6Q  17 January 2018   \n",
       "2  B01608M23I     3      5  AFA7TYS6KM6QLO5USYDOIDKUAMTA    16 March 2018   \n",
       "3  B01608M23I     3      5  AHKXO7XI2FSQYJEUID7H5TRMY4ZQ    27 March 2018   \n",
       "4  B01608M23I     3      5  AHA2E7FB7Y6QQQPZNM62R4CNACSA    21 March 2018   \n",
       "\n",
       "        title                                   comment           verified  \\\n",
       "0  Five Stars  Happy with product he loves play mobile.  Verified Purchase   \n",
       "1  Five Stars                                 excellent  Verified Purchase   \n",
       "2  Five Stars                   Grand daughter loves it  Verified Purchase   \n",
       "3  Five Stars                           Godson loves it  Verified Purchase   \n",
       "4        Good                                      Good  Verified Purchase   \n",
       "\n",
       "   helpful  pics  video      comment_id author_status  \n",
       "0        0     0      0   RTTUM0QU7HZWQ          none  \n",
       "1        0     0      0  R3FNSJ45BRF0X8          none  \n",
       "2        0     0      0   RE2CU061L9FBT          none  \n",
       "3        0     0      0  R1G8XD7M3NPF9M          none  \n",
       "4        0     0      0  R2YW9S0RZDA23M          none  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at a sample of the reviews\n",
    "example_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
